{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FA22_Lab10",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnyJFR3hPflIRBXck5pcZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hewp84/tinyml/blob/main/FA22_Lab10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 10: Machine Learning 2 - Machine Learning Implementation to Edge Device"
      ],
      "metadata": {
        "id": "FzYX9YitC-75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this last lab, we will implement a machine learning model to Raspberry Pi as an application of IIoT smart monitoring system to predict running conditions of the axial flow fan (AFF). This lab is broken down into three main sections: **1) Understanding and analyzing data, 2) Machine learning implementation to Raspberry Pi**, and **3) Building up the entire monitoring system.** The entire monitoring system and the data pipeline are illustrated in Figure 1. \n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure1.png?raw=true)\n",
        "\n",
        "*Figure 1 Schematic of entire monitoring system and data pipeline*\n",
        "\n"
      ],
      "metadata": {
        "id": "tBuiwIVuDDIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding and Analyzing Data"
      ],
      "metadata": {
        "id": "aONYrB-8DEke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Acceleration of the AFF\n",
        "\n",
        "In this section, we will go over the algorithm of the smart monitoring system and determine some variables for the smart monitoring system. The goal of the monitoring system for the AFF is to visualize in real-time 1) the vibration of the AFF, 2) the execution (‘ACTIVE’ or ‘STOPPED’), and 3) the prediction of running condition (‘NORMAL’ or ‘ABNORMAL’). As expected, the prediction of running conditions based on the machine learning model should be only done when the AFF is in an ‘ACTIVE’ state. If the AFF is not running, the prediction of anomaly does not mean anything. \n",
        "\n",
        "The algorithm and the flowchart of the AFF monitoring system is illustrated in Figure 2. This data flow will be repeated in a loop in your program. In Lab9, we developed an anomaly detection model (Autoencoder) for the second decision (“Is AFF normal?”). But we do not have a model for the first decision (“Is AFF running?”). Of course, a model to determine the running state (‘ACTIVE’ or ‘STOPPED’) can be developed by using machine learning approach as we performed in Lab9. However, let’s make the simplest model based on the measured vibration data. The idea is that when the AFF is running, the acceleration of each axis of the sensor must increase. Therefore, based on a simple logic such as a certain axis acceleration is lower than a specific acceleration value, a threshold on running, we can determine if the AFF is running or not. In other words, if the acceleration of a certain axis is higher than the threshold value, we can say the AFF is running, e.i., ‘Execution’ of AFF is ‘ACTIVE’. \n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure2.png?raw=true)\n",
        "\n",
        "*Figure 2 Flow chart of AFF monitoring system*\n",
        "\n",
        "The sample plots according to the various conditions of the AFF are in [APPENDIX](https://colab.research.google.com/drive/1DyKTOPtkSiUWEnzS9-gDQo_zrrfp1U7v#scrollTo=LfQ-rS3gD6Z3&line=1&uniqifier=1) at the end of this manual. On the time domain plots, you can see the RMS values according to the axis. As you can see, when the machine is stopped (Figure A. 2), the RMS (root mean square) accelerations are the minimum on the x- and y-axis. The sample data shows that it is reasonable to set the execution rms threshold of the x-axis as 1 m/s2. However, the rms threshold value may be different according to the AFF, and the sensor configurations. \n",
        "TASK 1\n",
        "\n",
        "First, **deploy ADXL345 sensor to the AFF as the same to Lab9**. To determine the rms threshold for the execution of the AFF, perform TASK 1 below. \n",
        "\n"
      ],
      "metadata": {
        "id": "p1ZljA4ADHFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1\n",
        "\n",
        "1.\tRun ‘lab10_sample1.py’ on Raspberry Pi to check the rms values on each axis according to the executions (‘STOPPED’ and ‘ACTIVE’) of the AFF. \n",
        "2.\tDetermine the rms threshold value of a specific axis. What are the rms value and the axis to determine if the AFF is running? \n",
        "3.\tModify ‘lab10_sample1.py’ so that you can see the execution (running state) of the AFF. \n",
        "  a. Use ‘if’ and ‘else’ statement. \n",
        "\n",
        "  b.\tThe example of the result is shown in Figure 3. \n",
        "\n",
        "  c.\tBy turning the AFF on and off repeatedly, confirm if your logic and the rms threshold is effective. \n",
        "\n",
        "  d.\tAttach the capture of Terminal or Thonny Shell to the report. \n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure3.png?raw=true)\n",
        "\n",
        "*Figure 3 Check the rms values and the execution of AFF*"
      ],
      "metadata": {
        "id": "8l_n5ykzDKHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Finding Minimum and Maximum Value of your Training Data Set\n",
        "\n",
        "In Lab9, you developed your own autoencoder model for the anomaly detection of the AFF. When training and validating the model, we used the normalized data set. Other than the threshold (mae, mean absolute error, loss) for the autoencoder model, when you implement the model to Raspberry Pi, you need to use the minimum and the maximum to normalize the extracted features in order to employ your model. The data flow in real time to predict running conditions of the AFF is illustrated in Figure 4. To normalize the input feature, we should know the minimum and the maximum values when training. Likewise, for the anomaly detection based on the autoencoder model, we should know the MAE loss threshold. \n",
        "\n",
        "To get the minimum and maximum values from the collected data, perform TASK 2. The ‘*lab10_sample2.py*’ is prepared based on ‘*lab9_ML_sample.ipynb*’ to check the minimum and maximum values of the input features. \n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure4.png?raw=true)\n",
        "\n",
        "*Figure 4 Real-time data flow for machine learning model to determine running conditions of AFF*\n"
      ],
      "metadata": {
        "id": "PBqx3gUZDLzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2\n",
        "\n",
        "1.\tRun ‘lab10_sample2.py’ on laptop to check the minimum and the maximum values of the extracted input feature of the model. \n",
        "\n",
        "  a.\tThe collected data sets must be in the same directory as the Python script. \n",
        "2.\tThe script is incomplete as is, so you need to complete the script. \n",
        "\n",
        "  a.\tVariables you need to complete: \n",
        "    * normal_data_file (line 43) \n",
        "    * abnormal_data_file (line 44) \n",
        "    * DIMENSION (line 57) iv. input_feature (line 79) \n",
        "3.\tAnswer the questions below. \n",
        "\n",
        "  a.\tWhat is the input feature? (Axis and type of feature among raw, time feature, frequency feature).  \n",
        "\n",
        "  b.\tWhat is the minimum value of the input feature? \n",
        "\n",
        "  c.\tWhat is the maximum value of the input feature? \n",
        "\n",
        "  d.\tWhat is the threshold (MAE loss) value from Lab9 for the trained model? \n"
      ],
      "metadata": {
        "id": "2UHZIlwyDSoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you are ready to implement the machine learning model using TensorFlow to Raspberry Pi. "
      ],
      "metadata": {
        "id": "IqYFtdMpi4lQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow implementation to Raspberry Pi"
      ],
      "metadata": {
        "id": "21ml8MK_DUJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Load and run TensorFlow model\n",
        "\n",
        "To implement the TensorFlow model which you developed and finally is selected in Lab 9 to Raspberry Pi, please follow the steps below. Please note that all required functions and the basic structure of the script are prepared in ‘lab10_ML_implementation.py’ on Brightspace. \n",
        "1.\tCreate a working directory on Raspberry Pi. \n",
        "2.\tCopy and paste ‘/model’ directory from laptop to the created directory of Raspberry Pi. \n",
        "3.\tCopy and paste ‘lab10_ML_implementation.py’ to the same directory of Raspberry Pi as below. \n",
        "  \n",
        "  ![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Image1.png?raw=true)\n",
        "\n",
        "  PERFORM TASK 3 to get the prediction result from the model and TASK 4 to complete the algorithm (Figure 2). \n",
        "\n"
      ],
      "metadata": {
        "id": "h-YVvtnlDXj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3\n",
        "\n",
        "1.\tOpen ‘lab10_ML_implementation.py’ and then modify it according to your ML model requirements \n",
        "\n",
        "  a.\tYou need to modify variables surrounded by asteriks in the part of the code below. E.g., `*example_var* = /sample/dir/` -> `example_var = /real/dir/`.\n",
        "\n",
        "2.\tRun the modified code, capture ‘Terminal’ or ‘Thonny Shell’ as Figure 5 and then attach it to the report. \n",
        "\n",
        "  a.\tBy turning the AFF on and off repeatedly, confirm if your model works well. \n",
        "3.\tAnswer to the questions below: \n",
        "\n",
        "  a.\tDoes the model work as expected? \n",
        "\n",
        "  b.\tWhat does the result of the model mean? \n",
        "\n",
        "  c.\tWhen the AFF stop, what is the result of the model? \n",
        "\n",
        "---\n",
        "\n",
        "**Python - Python3 (\"Main\" of 'lab10_ML_implementation.py')**\n",
        "\n",
        "```\n",
        "if __name__ == \"__main__\":\n",
        "    *model_path* = \"model/YourModelDirectory/\" # model file directory, you must change this!\n",
        "    model = tf.keras.models.load_model(model_path) # load the model from the path above\n",
        "    *threshold* = # threshold (MAE loss) for the ML model\n",
        "    *min_val* = # minimum value for normalization\n",
        "    *max_val* = # maximum value for normalization\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            x, y, z = measureData(acc, 1000) # x=x-axis, y=y-axis, z-axis acceleration array\n",
        "            *input_feature* = # your input feature\n",
        "            input_feature_normalized = tensorNormalization(input_feature, min_val, max_val) # normalized input feature\n",
        "            result = predict(model, *FINAL_FEATURE_INPUT*, threshold) # your model result: bool: True or False\n",
        "            now = datetime.datetime.now() # datetime now\n",
        "            print(\"{0}:Model result={1}, MAE loss={2:.4f}.\".format(now, result[0], result[1]))\n",
        "        except KeyboardInterrupt:\n",
        "            # to halt, press Ctrl + c\n",
        "            raise\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure5.png?raw=true)\n",
        "\n",
        "*Figure 5 The result of the ML model by TASK 3: ‘Terminal’ (left) and ‘Thonny’ (right)*\n",
        "\n"
      ],
      "metadata": {
        "id": "b0CEEADRDba1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 4\n",
        "\n",
        "1. To complete algorithm as Figure 2, modify the code (‘lab10_ML_implementation.py’) after performing \n",
        "TASK 1 and TASK 3.\n",
        "\n",
        "  a. When the AFF is running, the execution must be ‘ACTIVE’.\n",
        "\n",
        "  b. When the AFF is running without the attached putty, the condition must be ‘NORMAL’.\n",
        "\n",
        "  c. When the AFF is running with attached putty, the condition must be ‘ABNORMAL’.\n",
        "\n",
        "  d. When the AFF is NOT running, the execution and the condition must be ‘STOPPED’ and ‘UNAVAILABLE’, respectively.\n",
        "\n",
        "  e. Please note that the algorithm can be expressed as below:\n",
        "\n",
        "```\n",
        "if AFF is running:\n",
        "  execution = 'ACTIVE'\n",
        "  if AFF is normal:\n",
        "    condition = 'NORMAL'\n",
        "  else:\n",
        "    condition = 'ABNORMAL'\n",
        "else:\n",
        "  execution = 'STOPPED'\n",
        "  condition = 'UNAVAILABLE'\n",
        "\n",
        "```\n",
        "\n",
        "2. The example result is shown in Figure 6.\n",
        "\n",
        "  a. If you are not sure about how to determine the execution, review TASK 1.\n",
        "\n",
        "3. Attach both the capture of the result and the entire modified script to the report.\n",
        "\n",
        "4. Do your model and algorithm work as expected? Try change the running states (executions) and conditions.\n",
        "\n",
        "  a. If yes, please explain your test setup and the results.\n",
        "\n",
        "  b. If not, please explain the possible reasons and how to make it work.\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure6.png?raw=true)\n",
        "\n",
        "*Figure 6 The result of algorithm by TASK4: ‘Terminal’ (left) and ‘Thonny’ (right)*"
      ],
      "metadata": {
        "id": "6ctaKNZsDdFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Up THe Entire Monitoring System"
      ],
      "metadata": {
        "id": "xUVkE9HfDejQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: MTConnect Data Stream\n",
        "\n",
        "Let’s move on to the last step for the entire smart IIoT monitoring system. In this section, we will integrate each monitoring part we practiced in all previous labs into a single system to realize a real-time web-based monitoring system as Figure 1. In the previous section of this lab, we practiced the use of the machine learning model using TensorFlow on Raspberry Pi. In this part, let’s make MTConnect data stream including ML results based on the model. The schematic and the data items are shown in Figure 7. \n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure7.png?raw=true)\n",
        "\n",
        "*Figure 7 Schematic and data items of MTConnect*\n",
        "\n",
        "The examples of ‘agent.cfg’ and ‘device.xml’ are Figure 8 and Figure 9, respectively. You may have different XML data structure or data item definitions. Also, you can check the example of MTConnect agent running on the server computer: http://me597server1.ecn.purdue.edu:5000/.\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure8.png?raw=true)\n",
        "\n",
        "*Figure 8 front part of 'agent.cfg'*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure9.png?raw=true)\n",
        "\n",
        "*Figure 9 'device.xml'*"
      ],
      "metadata": {
        "id": "xcEebd91Dh82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 5\n",
        "\n",
        "1.\tRun MTConnect agent on Raspberry Pi with the XML structure and data items defined above. \n",
        "\n",
        "  a.\tPlease note that the command to run MTConnect agent is ‘*sudo ./agent*’ in the same directory of the execution file. \n",
        "\n",
        "  b.\tThe ‘*agent.cfg*’ and ‘*device.xml*’ must be in the same directory. \n",
        "\n",
        "  c.\tTo apply the schema and style in MTConnect agent, ‘*/schema*’ and ‘*/styles*' must be in the same directory as well. \n",
        "2.\tCapture ‘/current’ response from the MTConnect agent as Figure 10 and attach it to the report. "
      ],
      "metadata": {
        "id": "NhMqcjAUDkz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure10.png?raw=true)\n",
        "\n",
        "*Figure 10 MTConnect agent ‘/current’ response without MTConnect adapter*\n",
        "\n",
        "The next is to run MTConnect adapter. The sample MTConnect adapter based on the previous section is prepared, ‘lab10_adapter_sample.py’, on Brightspace. The sample is incomplete as is. **Please note that to run MTConnect adapter in this case, ‘data_item.py’, ‘mtconnect_adapter.py’ modules (on Brightspace), and ‘/model’ and sub-directory for your autoencoder model must be in the same directory of the MTConnect adapter Python script**. Perform TASK 6 to finish MTConnect data stream."
      ],
      "metadata": {
        "id": "esEEAl51t69u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 6\n",
        "\n",
        "1.\tComplete the sample script (*‘lab10_adapter_sample.py’*) and then run MTConnect adapter. \n",
        "\n",
        "  a.\tYou need to modify the variables wrapped in asteriks below. Also, you need to change the '???' parameters to the appropriate parameters. \n",
        "\n",
        "  b.\tIf you have a different XML structure and data items from the example, you must change other parts of the sample script as well. \n",
        "\n",
        "2.\tCapture ‘/current’ response from the MTConnect agent as Figure 11 and attach it to the report. \n",
        "\n",
        "  a.\tConfirm if every data item works as expected by running the AFF. \n",
        "\n",
        "---\n",
        "\n",
        "**Python - Python3 (\"Main of 'lab10_adapter_sample.py')**\n",
        "\n",
        "```\n",
        "if __name__ == \"__main__\":\n",
        "    *model_path* = \"model/YourModelDirectory/\" # model file directory, you must change this!\n",
        "    *model* = tf.keras.models.load_model(model_path) # load the model from the path above\n",
        "    *threshold* =  # float: threshold (MAE loss) for the ML model\n",
        "    *min_val* =  # float: minimum value for normalization\n",
        "    *max_val* =  # float: maximum value for normalization\n",
        "    \n",
        "    # start MTConnect Adapter\n",
        "    MTConnectAdapter(???, ???) # Args: host ip, port number\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Python - Python3 (Part of \"MTConnectAdapter\" class of 'lab10_adapter_sample.py')**\n",
        "\n",
        "```\n",
        "    def adapter_stream(self):\n",
        "        while True:\n",
        "            try:\n",
        "                x, y, z = measureData(acc,1000) # x=x-axis, y=y-axis, z-axis acceleration array\n",
        "                *input_feature* = # your input feature\n",
        "                input_feature_normalized = tensorNormalization(input_feature, ???, ???) # normalized input feature\n",
        "                result = predict(model, *FINAL_FEATURE_INPUT*, threshold)\n",
        "                x_rms = timeFeatures(x)[2] # calculate rms of x-axis\n",
        "                *y_rms* =  # calculate rms of y-axis\n",
        "                *z_rms* =  # calculate rms of z-axis\n",
        "                mae = result[?] # mean absolute error (loss) of the model\n",
        "                \n",
        "                if *AFF is running*:\n",
        "                    execution = 'ACTIVE'\n",
        "                    if *AFF is normal*:\n",
        "                        condition = 'NORMAL'\n",
        "                    else:\n",
        "                        condition = 'ABNORMAL'\n",
        "                else:\n",
        "                    execution = 'STOPPED'\n",
        "                    condition = 'UNAVAILABLE'\n",
        "\n",
        "                now = datetime.datetime.now() # get current data time\n",
        "                \n",
        "                self.adapter.begin_gather() # start to collection\n",
        "                \n",
        "                self.Xacc.set_value(str(???))\n",
        "                self.Yacc.set_value(str(???))\n",
        "                self.Zacc.set_value(str(???))\n",
        "                self.execution.set_value(str(???))\n",
        "                self.condition.set_value(str(???))\n",
        "                self.mae.set_value(str(???))\n",
        "                \n",
        "                self.adapter.complete_gather() # end of collection\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure11.png?raw=true)\n",
        "\n",
        "*Figure 11 MTConnect agent ‘/current’ response with MTConnect adapter*"
      ],
      "metadata": {
        "id": "oLj0j3JZDmwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: MySQL Database\n",
        "\n",
        "Let’s shift gears to the data aggregation for the MySQL database. Recall Lab7 and Lab8 for this part. The information of the MySQL server and database is below. \n",
        "* DNS: me597server1.ecn.purdue.edu \n",
        "* Port: 3306 \n",
        "* Account: yourname (e.g., johndoe) \n",
        "* Database: ME597Spring22 \n",
        "* Table: yourname_lab7 (e.g., johndoe_lab7) \n",
        "\n",
        "The data collector Python script (‘mtconnect_collector.py’ on Brightspace) using XML parsing is prepared for you as Lab8. Perform TASK 7 to collect all data stream from the MTConnect agent. \n",
        "\n"
      ],
      "metadata": {
        "id": "eowdofpFDonT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 7\n",
        "\n",
        "1.\tRun the sample data collector Python script (‘mtconnect_collecor.py’) on either laptop or Raspberry Pi to store all data stream from the MTConnect agent. \n",
        "\n",
        "  a. You need to modify some front parts that are wrapped in asteriks below. \n",
        "2.\tCheck if data is being stored using ‘MySQL Workbench’ on laptop.  \n",
        "3.\tCapture the result grid as Figure 12 and then attach it to the report. \n",
        "\n",
        "---\n",
        "\n",
        "**Python - Python3 ('mtconnect_collector.py')**\n",
        "\n",
        "```\n",
        "# Credential\n",
        "HOST = 'MySQL HOST DNS' # MySQL server host DNS\n",
        "PORT = 3306 # MySQL server port number\n",
        "USER = 'your account' # MySQL account name\n",
        "PASSWORD = 'your password' # Password of the account\n",
        "DB = 'Database name' # DB name\n",
        "TABLE = 'your table name' # table name\n",
        "## Credential\n",
        "\n",
        "## MTConnect info.\n",
        "agent = \"agent host\"\n",
        "agent_port = \"agent port number\"\n",
        "url_current = \"http://\"+agent+\":\"+agent_port+\"/current\"\n",
        "MTCONNECT_STR = ET.fromstring(requests.get(url_current).content).tag.split(\"}\")[0]+\"}\"\n",
        "print(\"MTConnect string header is {}.\".format(MTCONNECT_STR))\n",
        "## MTConnect info\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure12.png?raw=true)\n",
        "\n",
        "*Figure 12 Result grid of stored data in MySQL workbench*"
      ],
      "metadata": {
        "id": "2FhR5PFBDrPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6: MySQL Database #2\n",
        "\n",
        "Finally, let’s create a web-based dashboard using Grafana. Recall Lab8 activities. The information of the \n",
        "Grafana server domain and the port number is below. Log in the Grafana server in a web-browser of laptop. \n",
        "\n",
        "* DNS: me597server1.ecn.purdue.edu \n",
        "* Port: 3000 \n",
        "* Grafana web page URL: http://me597server1.ecn.purdue.edu:3000/ \n",
        "* Account (username): yourname (e.g., johndoe) \n",
        "\n",
        "Go to ‘*Lab10 sample*’ dashboard in ‘*0_SAMPLE*’ dashboard folder. This dashboard (Figure 13) is what you should finally create. \n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_Figure13.jpg?raw=true)\n",
        "\n",
        "*Figure 13 Sample Grafana dashboard (0_SAMPLE/Lab10 sample)*\n"
      ],
      "metadata": {
        "id": "8xfeK5hzDyAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 8\n",
        "\n",
        "1.\tCreate a dashboard as Figure 13 in your folder created in Lab 8 and save it as the dashboard name of ‘lab10 your name’, e.g., ‘lab10 john doe’. \n",
        "2.\tCapture the dashboard and attach it to the report. \n",
        "3.\tAfter creating the dashboard, confirm if the entire monitoring system works in real-time by refreshing the dashboard. \n"
      ],
      "metadata": {
        "id": "S90QMNdHD2i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deliverable\n",
        "\n",
        "1.\tPerform all Tasks and submit your Lab10 report on Brightspace within a week. \n",
        "2.\tSummarize Lab 10 what you performed and learned. \n",
        "  * Use any photos, figures, tables, and equations if needed. \n"
      ],
      "metadata": {
        "id": "ZPku01tjD4Ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA1.jpg?raw=true)\n",
        "\n",
        "*Figure A. 1 Hardware configuration of ADXL345 sensor and AFF*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA2.jpg?raw=true)\n",
        "\n",
        "*Figure A. 2 Acceleration data when AFF is STOPPED: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA3.jpg?raw=true)\n",
        "\n",
        "*Figure A. 3 Acceleration data when AFF is running in normal with 1800 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA4.jpg?raw=true)\n",
        "\n",
        "*Figure A. 4 Acceleration data when AFF is running in normal with 2150 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA5.jpg?raw=true)\n",
        "\n",
        "*Figure A. 5 Acceleration data when AFF is running in normal with 2500 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA6.jpg?raw=true)\n",
        "\n",
        "*Figure A. 6 Acceleration data when AFF is running in normal with 2750 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA7.jpg?raw=true)\n",
        "\n",
        "*Figure A. 7 Acceleration data when AFF is running in normal with 3000 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA8.jpg?raw=true)\n",
        "\n",
        "*Figure A. 8 Acceleration data when AFF is running in abnormal with 1800 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA9.jpg?raw=true)\n",
        "\n",
        "*Figure A. 9 Acceleration data when AFF is running in abnormal with 2150 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA10.jpg?raw=true)\n",
        "\n",
        "*Figure A. 10 Acceleration data when AFF is running in abnormal with 2500 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA11.jpg?raw=true)\n",
        "\n",
        "*Figure A. 11 Acceleration data when AFF is running in abnormal with 2750 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "![picture](https://github.com/hewp84/tinyml/blob/main/img/L10_FigureA12.jpg?raw=true)\n",
        "\n",
        "Figure A. 12 Acceleration data when AFF is running in abnormal with 3000 rpm: time domain (left) and frequency domain (right)*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LfQ-rS3gD6Z3"
      }
    }
  ]
}