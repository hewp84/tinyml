{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_1_TinymlParadigm",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hewp84/tinyml/blob/main/2_1_TinymlParadigm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Shift of Machine Learning\n",
        "\n",
        "##Learning Goal\n",
        "You will be able to apply the Machine Learning paradigm to train a model\n",
        "\n",
        "You will be able to evaluate the loss function in a trained model\n",
        "\n",
        "You will be able to contrast between underfitted, overfitted, and well-fitted trained models"
      ],
      "metadata": {
        "id": "2bfsLoHuhhRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning figures things out on its own. Compare the traditional programming paradigm to the machine learning paradigm below."
      ],
      "metadata": {
        "id": "AfGrrxwbPvG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?id=1-suVpteJGitubqtsnSe0Cq8mUb5v7qvq)\n",
        "<font size='1'>Figure 1: TP vs ML paradigm<sup>1</sup></font>"
      ],
      "metadata": {
        "id": "m6o_HGX8Wst2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a Machine Learning algorithm to solve a problem, there are two key features. (1) it will take a piece of **data** and (2) will take a **guess** at the **answer**. Then, it will *optimize* its **guess** until it gets the correct **answer**. The concept of *loss* will be what helps to *optimize* the **guess**. \n",
        "\n",
        "To illustrate, the below diagram shows the general machine learning process."
      ],
      "metadata": {
        "id": "mlV5fjIzXQwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1024NYwnl_LIHRl5YbdsL-ZAjxcmAFybu\" width=\"825\" height=\"325\"/>\n",
        "<br><font size='1'>Figure 2: ML paradigm process<sup>1</sup></font>\n"
      ],
      "metadata": {
        "id": "Nw36UX7Objia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulating a Machine Learning model to understand *loss*\n",
        "\n",
        "Here we will iterate through, by trial and error, different **rules** (the \"output\" or \"result\" in the ML paradigm) to understand why machine learning is so powerful, and principly lies on the concept of **loss**<br>\n",
        "<br>\n",
        "We are given a set of **data** in the form of **x** values that we will work with. And, a set of answers (**y** values) that correspond to the data.\n"
      ],
      "metadata": {
        "id": "2IEdL8P1TAUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [-1, 0, 1, 2, 3, 4]                    # input values\n",
        "y = [-3, -1, 1, 3, 5, 7]                   # output, or, \"answer\" values"
      ],
      "metadata": {
        "id": "ChErAapYYhMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For any piece of **data** (**x**-value, input), we should be able to figure out the **answer**. <br>"
      ],
      "metadata": {
        "id": "Ne3bahpOUPNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Guessing the rule ourselves\n",
        "We need a **rule** (the function that will provide the answer for the given input). For our simple example, this rule will be a linear function in the form of **y=mx+b**.\n",
        "\n",
        "Shown below is a slider to adjust different 'm' and 'b' values so that we can demonstrate how good or bad rules are at predicting (drawing inferences) about the answer.\n",
        "\n",
        "By default, m = 3 and b = -1. <br>\n",
        "\n",
        "#### The graph below illustrates loss\n",
        "The <font color=\"green\"> green </font> line shows the real and accurate values that correlate our **x** to our **y**. The <font color=\"red\"> red </font> line shows the values that are predicted based on our rule that is defined by our slider. The **loss** is visualized by the space between the two lines.\n",
        "<br>\n",
        "\n",
        "Run the cell below and then play around by changing the **m** value on the slider and the graph will update automatically."
      ],
      "metadata": {
        "id": "UU2NKWnBWFA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Change the values to explore how loss changes {display-mode:\"form\", run:\"auto\"}\n",
        "\n",
        "m = 2       #@param {type:\"slider\", min:-25, max:25, step:1}\n",
        "b = 1       #@param {type:\"slider\", min:-25, max:25, step:1}\n",
        "\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plot\n",
        "plot.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "\n",
        "# the following calculates e\n",
        "guessY = []\n",
        "for iX in x:\n",
        "  iY = (m*iX) + b\n",
        "  guessY.append(iY)\n",
        "\n",
        "total_square_error = 0\n",
        "for i in range(0, len(y)):\n",
        "  square_error = (y[i] - guessY[i]) ** 2\n",
        "  total_square_error += square_error\n",
        "\n",
        "print(\"The predicted Y is: \" + str(guessY) + \"\\n\")\n",
        "print(\"The loss for this guess is: \" + str(math.sqrt(total_square_error)))\n",
        "\n",
        "fig, ax = plot.subplots()\n",
        "ax.plot(x, y, marker='o', color='green', label='Real values')\n",
        "ax.plot(x, guessY, marker='^', color='red', label='Predicted values')\n",
        "leg = ax.legend()\n",
        "ax.legend(loc='upper left')\n",
        "\n",
        "total_square_error = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "RFl9HovEjNDU",
        "outputId": "af5f8daf-d3c7-4757-c338-fd692e116198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted Y is: [-1, 1, 3, 5, 7, 9]\n",
            "\n",
            "The loss for this guess is: 4.898979485566356\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Reality\n",
        "\n",
        "This is a very trivial process. It is not too hard to find that m=2 and b=-1 (y = 2x-1) is the correct rule to match the **x** values to the **y** values. \n",
        "\n",
        "<br>\n",
        "\n",
        "However, with more complex data, figuring out those values for the rule is not so easy. Machine learning helps us out. By taking a guess, measuring our accuracy, and optimizing our guess *again* and *again*, <font color='red'>we can produce a **model** that will provide accurate **inferences** from any **data**. </font>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=103DzWjmdOgRtutiXVOtxZMEIl_0Oiy84\" width=\"490\" height=\"159\"/> <br>\n",
        "<font size='1'>Figure 3: A \"model\"<sup>1</sup></font>"
      ],
      "metadata": {
        "id": "5VPV_yjWkkX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning\n",
        "\n",
        "Instead of simulating the machine learning process as we did above, now we will let the machine do the guessing, measuring, optimizing, and *learning*.\n",
        "\n",
        "The following block of code defines a simple neural network that will utilize the same **x** and **y** value. Each **epoch** that it lists is a **guess** that it took. For each epoch it calculates the **loss** and shows that as well. On each new guess that the ML algorithm takes, it tries to further minimize loss down to 0."
      ],
      "metadata": {
        "id": "IoB29vdZTiEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "eval_js('google.colab.output.setIframeHeight(\"270\")')\n",
        "\n",
        "# define a neural network with one neuron\n",
        "# for more information on TF functions see: https://www.tensorflow.org/api_docs\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "# use stochastic gradient descent for optimization and\n",
        "# the mean squared error loss function\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# define some training data (xs as inputs and ys as outputs)\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "# fit the model to the data (aka train the model)\n",
        "#@title ####Enter the number of epochs to train the model on and explore how the final loss value changes {run: \"auto\"}\n",
        "#@markdown ----\n",
        "#@markdown <br>\n",
        "Epochs = 500 #@param [10, 25, 50, 100, 500] {type: \"raw\"}\n",
        "\n",
        "history = model.fit(xs, ys, epochs=int(Epochs))\n",
        "\n",
        "#code for the loss graph\n",
        "train_loss = history.history['loss']\n",
        "#plots the x value from 0 to the # of epochs, loss values on the y-axis\n",
        "plot.plot((range(0, Epochs)), train_loss, 'g', label='Training loss')\n",
        "plot.xlabel('Epochs')\n",
        "plot.ylabel('Loss')\n",
        "plot.legend()"
      ],
      "metadata": {
        "id": "8QDC7y8voI5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "30a59519-49f3-4de6-d597-b599f1f64544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 7.7980\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.3312\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 5.1732\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.2581\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.5343\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.9611\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.5064\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.1451\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8572\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6273\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4429\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2946\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1746\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0770\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9970\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.9311\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.8762\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.8300\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.7908\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7571\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.7279\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7021\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6792\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6586\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6398\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6225\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6065\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.5915\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.5773\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5639\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5511\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5388\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5269\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5155\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5044\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4937\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4832\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4731\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4632\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4535\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.4441\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4349\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4259\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4171\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4085\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4000\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3918\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3837\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3758\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3681\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3605\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3531\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3459\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3387\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.3318\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3250\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.3183\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3118\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3053\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2991\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2929\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2869\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2810\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2752\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2696\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2641\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2586\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2533\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2481\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2430\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2380\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2331\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2283\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2237\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2191\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2146\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2102\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2058\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2016\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1975\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1934\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1894\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1855\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1817\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1780\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1743\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1708\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1673\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1638\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1605\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1572\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1539\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1508\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1477\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1446\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1417\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1388\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1359\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1331\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1304\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1277\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1251\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1225\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1200\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1175\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1151\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1128\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1104\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1082\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1059\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1038\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1016\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0996\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0975\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0955\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0935\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0916\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0897\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0879\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0861\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0843\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0826\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0809\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0792\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0776\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0760\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0744\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0729\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0714\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0700\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0685\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0671\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0657\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0644\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0631\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0618\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0605\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0593\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0580\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0568\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0557\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0545\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0534\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0523\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0512\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0502\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0492\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0481\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0472\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0462\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0452\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0443\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0434\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0425\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0416\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0408\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0399\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0391\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0383\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0375\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0368\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0360\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0353\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0345\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0338\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0331\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0325\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0318\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0311\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0305\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0299\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0293\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0287\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0281\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0275\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0269\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0264\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0258\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0253\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0248\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0243\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0238\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0233\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0228\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0223\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0219\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0214\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0210\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0206\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0201\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0197\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0193\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0189\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0185\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0182\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0178\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0174\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0171\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0167\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0164\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0160\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0157\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0154\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0151\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0148\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0144\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0142\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0139\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0136\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0133\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0130\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0128\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0122\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0120\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0117\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0115\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0113\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0110\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0108\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0106\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0104\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0102\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0099\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0097\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0095\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0093\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0092\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0090\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0088\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0084\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0082\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0079\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0078\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0076\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0074\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0073\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0071\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0070\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0068\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0067\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0066\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0064\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0063\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0062\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0060\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0058\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0057\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0056\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0054\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0053\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0052\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0051\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0050\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0049\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0047\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0044\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0042\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0042\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0041\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0040\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0039\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0038\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0037\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0037\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0036\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0035\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0035\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0034\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0033\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0032\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0032\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0030\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0030\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0023\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0022\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.9321e-04\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.7281e-04\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.5282e-04\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.3325e-04\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.1408e-04\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.9531e-04\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.7692e-04\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.5890e-04\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.4126e-04\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2398e-04\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.0706e-04\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9048e-04\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7425e-04\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.5834e-04\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.4276e-04\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2751e-04\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1257e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9793e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8359e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6955e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.5580e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4233e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 6.2913e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.1621e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.0355e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9116e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 5.7901e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.6712e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.5547e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.4406e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.3289e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.2194e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.1122e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.0072e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.9043e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8036e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7049e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.6083e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5136e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4209e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 4.3301e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2411e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1540e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.0687e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9851e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9033e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8231e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7446e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.6677e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5923e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5185e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4462e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3755e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3061e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2382e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1717e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.1065e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0427e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9802e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.9190e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8591e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8003e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7428e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6865e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6313e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5772e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5243e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4725e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4217e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3719e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3232e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2755e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.2287e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.1830e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.1381e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0942e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0512e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0091e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9678e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9274e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8878e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.8490e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8110e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.7738e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7374e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7017e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6668e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.6325e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5990e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.5661e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5340e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5025e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4716e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4414e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4118e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3828e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3544e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.3265e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.2993e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2726e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2465e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2209e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1958e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1712e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1472e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1236e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1005e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0779e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0558e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0341e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0129e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.9207e-05\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.7168e-05\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.5172e-05\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.3217e-05\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.1303e-05\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.9426e-05\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.7590e-05\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.5791e-05\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.4028e-05\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2302e-05\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.0613e-05\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.8957e-05\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.7335e-05\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.5747e-05\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.4190e-05\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2667e-05\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1173e-05\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9711e-05\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8280e-05\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6877e-05\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.5504e-05\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4158e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.2839e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.1549e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0285e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.9047e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.7833e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.6646e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.5482e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.4343e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.3227e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2133e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.1061e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0013e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.8986e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.7980e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6995e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.6030e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.5084e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4158e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.3249e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2361e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1492e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.0639e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9804e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.8987e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8186e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7401e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.6633e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.5881e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5144e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4422e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3715e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3022e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2344e-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a6e597fc41ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#plots the x value from 0 to the # of epochs, loss values on the y-axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a screenshot from the above code to demonstrate where a model is fit. Given a set of data, if there are too few **iterations** (epochs) of the training algorithm, the model will be **underfit** while if there are too many, the model is **overfit**."
      ],
      "metadata": {
        "id": "aWz6PWciiLQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=103kynKaai_d2fiC2543aUk0F9pvN4JtB\" width=\"480\" height=\"333\"/> <br>\n",
        "<font size='1'>Figure 4: Model Fitness<sup>2</sup></font>"
      ],
      "metadata": {
        "id": "wNTcUNIwhrUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Underfiting** our model will be detrimental because it will not make accurate inferences on **any** data. <br> </br>\n",
        "**Overfitting** is also detrimental because the inferences will be **too good**. Meaning, given the data we provide, it will perform very well. But, when we apply our model to other data (real-world data), it will make **poor inferences**. Our model will be so highly tuned to *only* the data we gave."
      ],
      "metadata": {
        "id": "NhxG2JgRipJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, there is a graph that charts how the ML minmized loss over time. \n",
        "\n",
        "Machine Learning took a **guess** at what the **rule** should be, then using **loss**, it was able to measure its **accuracy**. Depending on how accurate the guess was, it **optimized** its rule to make more guesses and bring the loss as close to zero **as possible**.\n",
        "\n",
        "### Resultingly\n",
        "a **model** was created. We went through the process of training the model, and now it will use its defined set of **rules** (that it learned) to take input and **produce an inference**, or, a best answer that fits the input data."
      ],
      "metadata": {
        "id": "4fNCl8tGFZ49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets go a try to model more complex data sets in the [next page](Simple_Model.ipynb)"
      ],
      "metadata": {
        "id": "hMnIGKE1sBL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br> </br>\n",
        "<br> </br>\n",
        "**References** <br>\n",
        "1: https://github.com/tinyMLx/courseware/blob/master/edX/slides/2-1-1.pdf <br>\n",
        "2: https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/\n"
      ],
      "metadata": {
        "id": "QuaEwkR2ITLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tL_tOYdCV2PS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}